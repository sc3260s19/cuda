{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory Bandwidth-Bound Problems and GPUs\n",
    "\n",
    "GPUs are generally thought of as an accelerator for compute-bound applications, but they also offer large performance boosts for problems that are memory bandwidth-bound: \n",
    "\n",
    "<img src=\"https://docs.nvidia.com/cuda/cuda-c-programming-guide/graphics/memory-bandwidth.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "The peak theoretical bandwidth of a NVIDIA grapchis card compared to an Intel Xeon CPU are shown in the table below.\n",
    "\n",
    "| NVIDIA Pascal Titan X  |  Intel Xeon E5-2623 v4 |\n",
    "|:-:|:-:|\n",
    "|  480 GB/s | 68 GB/s  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Addition Example\n",
    " \n",
    "A simple element-wise vector addition is a good example of a common memory-bound function:\n",
    "\n",
    "```C\n",
    "void vec_add(float * A, float * B, float * C, int N)\n",
    "{\n",
    "   for (int i=0; i<N; i++)\n",
    "      C[i] = A[i] + B[i];\n",
    "}```\n",
    "This function consists of a total of N floating-point operations and 3*N memory access operations (2 reads and 1 write per array element), so memory accesses are likely to be the bottleneck, rather than mathematical operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks\n",
    "\n",
    "Below are results of running various benchmarks on the GPU and CPU hardware shown in the table above. Note that benchmarks were performed on a system consisting of a dual-socket motherboard with 2 processors (8 physical cores total). We benchmark a single-GPU implementation, single CPU core implementation, and a multithreaded CPU implementation. In all cases the effective bandwidth is calculated by dividing the total bytes involved in memory access operations divided by the total execution time of a element-wise vector addition function/kernel. This is a lower bound on the true bandwidth as some time will be spent by the function/kernel performing the addition operation. A value of N=1e6 is used in all cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parallel Implementation</th>\n",
       "      <th>Bandwidth (GB/s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Serial</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OpenMP - 2 cores</td>\n",
       "      <td>2.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OpenMP - 4 cores</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OpenMP - 8 cores</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CUDA - tiled</td>\n",
       "      <td>201.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Parallel Implementation   Bandwidth (GB/s)\n",
       "0                  Serial                4.4\n",
       "1        OpenMP - 2 cores                2.8\n",
       "2        OpenMP - 4 cores                2.6\n",
       "3        OpenMP - 8 cores                2.7\n",
       "4            CUDA - tiled              201.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"bmarks.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The GPU performance blows the CPU out of the water. This is not unexpected given the theoretical bandwidth numbers shown above, however it is a powerful example of how GPUs can yield massive speedups for the simplest of memory-bound problems. Note that CUDA applications also require expensive operations in which data is moved from the host to device memory, which is not accounted for in the numbers above. This is purely a measure of bandwidth after any necessary data transfers have occured. \n",
    "\n",
    "Note that we are getting nowhere near the peak theoretical performance on the GPU or CPU. Part of that can be explained by our method for measuring bandwidth (we did not subtract time spent by the CPU performing addition operations). \n",
    "\n",
    "It's also noteworthy that CPU-based multithreading does not improve performance for this simple problem. In fact, effective bandwidth actually drops slightly with multiple threads/cores involved. This is because threads share hardware resources for accessing memory, resulting in a classic Von Neumann bottleneck scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
